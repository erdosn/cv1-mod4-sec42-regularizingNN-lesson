{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ♫ harder, better, faster, stronger ♫\n",
    "\n",
    "-written by Victor Geislinger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can optimize our model to run faster and better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some useful resources on optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keras documentation & discussion: https://keras.io/optimizers/\n",
    "- Excellent blog post on optimizations: http://ruder.io/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing Gradient Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When our gradients are too small we can run into a few issues when training our model (adjusting weights and biases):\n",
    "- Slow training\n",
    "- Local minimum problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions (from before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to change the gradient, is change the activation function that produces different gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid local minimums, what if we just randomly initialize ourselves somewhere on the cost function?\n",
    "\n",
    "We'll have to run the same model multiple times, but we start off with different weights and can help avoid falling into a local minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "\n",
    "- Power over the hill of local min\n",
    "- Dampens oscillations\n",
    "- Average over the past steps (decay the old steps)\n",
    "    + $S_n = S_n + \\beta \\cdot S_{n-1} + \\beta^2 \\cdot S_{n-2} + \\cdots$ \n",
    "    + $S_n = S_n + \\beta \\cdot \\big ( S_{n-1} - S_n \\big )$ \n",
    "    + $S_n = \\beta \\cdot S_{n-1} + \\big ( 1- \\beta \\big ) \\cdot S_n$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speeding Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows speedier training so no one feature will overpower the direction down the hill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of taking a long time through the whole process with dataset (careful step), go through part of the dataset quicker (quick, \"drunken\" steps)\n",
    "\n",
    "\"Stochastic\" == Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "\n",
    "1. Take a random set (batch)\n",
    "2. Feedforward batch through model\n",
    "3. Calculate error (loss) from batch\n",
    "4. Adjust weights via backpropogagtion\n",
    "5. Repeat with all points/batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea is that:\n",
    "\n",
    "- **high learning rate** value --> we move fast but possibly skip over the local minimum\n",
    "- **low learning rate** value --> we move slowly, maybe never getting there\n",
    "\n",
    "So there are advantages and disadvantages for each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=40% src='images/why-not-both.jpg'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have more epochs, we should approach our answer. We can decrease $\\alpha$ as we complete epochs, so we (hopefully) do the following: \n",
    "- If steep (large) gradient, we use a large $\\alpha$\n",
    "- If level (small) gradient, we use a small $\\alpha$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
